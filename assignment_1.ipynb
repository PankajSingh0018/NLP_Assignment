{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment No 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1 - Explain One-Hot Encoding "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer-** - \n",
    "\n",
    "One-hot encoding is a data preprocessing technique used to convert the textual data into a numeric form, commonly employed in Natural language processing. \n",
    "\n",
    "In one-hot encoding, for each unique word within a document or a corpus, a binary vector is created. This binary vector contains only 0s and 1s, where each position in the vector corresponds to a specific word, and a '1' is placed in the position that corresponds to the word's presence in the text, while all other positions are filled with '0s'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Explain Bag of Words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer-**\n",
    "\n",
    "Bag of word is data-preprocessing technique used in natural language processing task to convert the textual data into numerical representation.\n",
    "\n",
    "In Bag of words approach, a text document is transformed into a vector where each dimension represent a unique word in the corpus. The frequency of each word's occurence in the document is used as the value in the corresponding dimension of the vector.\n",
    "\n",
    "This method capture the word frequency information but ignores the order and structure of the words in the text.\n",
    "\n",
    "BOW is particulary useful for tasks like classification and information retrieval, where the focus is on the presence and frequency of words rather than their order.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.\tExplain Bag of N-Grams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer-** \n",
    "\n",
    "N-gram is a text preprocessing technique used to represent text data in numerical format. Instead of looking at individual words like the Bag of Words (BOW) approach, n-grams consider sequence of 'n' consecutive words in the text.\n",
    "\n",
    "The sequence is called the 'n-grams'. For example , if 'n' is set to 2(biagram), the sentence 'I love Ice cream' would be spillted into - \n",
    "\n",
    "'I love' and 'love Ice' and 'Ice cream'.\n",
    "\n",
    "N-grams capture not only the individual words but also the context and the relationships between words. This helps in preserving some level of syntax and semantics in the numerical representation.\n",
    "\n",
    "A sliding window technique is often used to create the n-grams by moving a fixed sized window through the text and extracting sequences of 'n' words within that window.\n",
    "\n",
    "This technique is valuable in tasks like text classification, sentiment analysis, and machine translation, where understanding word relationshops and context is important.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.\tExplain TF-IDF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer-**\n",
    "\n",
    "\"TF-IDF\" stands for Term Frequency- Inverse Document Frequency, is a text preprocessing technique used to convert text into a numerical representation that helps in understanding the importance of words in documents or a corpus.\n",
    "\n",
    "TF- IDF consists of Two main parts-- \n",
    "\n",
    "1- **Term Frequency** - This part calculates the frequency of each word within a specific document. It emphasizes words that appears more frequently in the document. \n",
    "\n",
    "It is calculated as the number of times a word appears in a document divided by the total number of words in that document. Essentially, it measures how often a word occurs within a particular document.\n",
    "\n",
    "2-**Inverse Document Frequency** - This part measures the importance of a word in the entire corpus. It takes into account how frequently a word appears across all documents in the corpus. Words that are common across all documents given less importance, while words that are unique to specific documents are given more importance.\n",
    "\n",
    "IDF is calculated as the logarithm of the total number of documents divided by the number of documents containing the word.\n",
    "\n",
    "The final TF-IDF score for a word in a document is the product of TF and IDF. \n",
    "\n",
    "This approach is useful in information retrieval, text mining and search engine ranking, as it helps identify the significance of words in distinguishing one document from another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.\tWhat is OOV problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer-**\n",
    "\n",
    "OOV stands for Out-of-Vocabulary, it refers to the a common challenge in natural language processing where a word that is not present in the vocabulary or training data is encountered during the text processing or analysis. This typically happens when a new or rare word appears in a document that was not seen during the model training phase.\n",
    "\n",
    "When an OOV word is encountered, it poses a challenge because the model or the system may not know how to handle it. Traditional language models and vector representations (eg. word embeddings) have a fixed vocabluary size, and any words outside this vocabulary are considered OOV.\n",
    "\n",
    "To address the OOV problem, various techniques can be employed, such as:\n",
    "\n",
    "1- Tokenization - Breaking down the text into smaller units like subwords or characters, which reduces the chances of encountering OOV words.\n",
    "\n",
    "2- Adding Unknown Tokens- Using a special token( often \"<UNK>\") to represent all OOV words. This helos maintain the integrity of the model's input but may limit the understanding of rare words.\n",
    "\n",
    "3- Subword Model - Utilizing models like Byte-Pair Encoding(BPE) or WordPiece that can handle subword units, making it less likely to encounter OOV words.\n",
    "\n",
    "4- Word Embedding Models - Training word embedding on a larger and more diverse corpus to include a broader vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.\tWhat are word embeddings?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer-** \n",
    "\n",
    "Word Embeddings:\n",
    "\n",
    "Word embeddings are a type of word representation in natural language processing (NLP) that aim to capture the semantic relationships between words based on their context within a large text corpus. These embeddings are numerical vectors that map words from a high-dimensional space (the vocabulary) into a continuous, lower-dimensional space. The key idea behind word embeddings is that words with similar meanings should have similar vector representations.\n",
    "\n",
    "Word embeddings are typically learned through unsupervised learning techniques, such as Word2Vec, GloVe, or FastText, by training on massive text corpora. During training, the model analyzes the co-occurrence patterns of words. Words that often appear together in similar contexts will have vectors that are closer together in the embedding space.\n",
    "\n",
    "Word embeddings have become a fundamental component of many NLP applications, as they allow models to capture and leverage semantic information, making them more effective at tasks like sentiment analysis, machine translation, and document classification. These embeddings can be used to initialize the word representations in various deep learning models, providing a valuable foundation for understanding and generating human language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.\tExplain Continuous bag of words (CBOW)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer-**\n",
    "\n",
    "Continuous Bag of Words (CBOW) is a type of word embedding model used in natural language processing (NLP) and deep learning. It's one of the techniques used to convert words into numerical vectors, which can be used for various NLP tasks, including text classification, sentiment analysis, and machine translation.\n",
    "\n",
    "CBOW operates on the principle of predicting a target word based on its context, i.e., the words that surround it in a sentence. Here's a simplified explanation of how CBOW works:\n",
    "\n",
    "**Data Preparation:** The training data consists of a large corpus of text. Each word in the corpus is represented as a unique numerical identifier (e.g., an index).\n",
    "\n",
    "**Context Window:** CBOW defines a context window of a fixed size around each target word. This context window includes a certain number of words that come before and after the target word in a sentence.\n",
    "\n",
    "**Word Embedding:** For each word in the context window, CBOW looks up its corresponding word vector from an embedding matrix. This embedding matrix contains pre-trained word vectors or is initialized with random values and learned during training.\n",
    "\n",
    "**Vector Aggregation:** The word vectors from the context window are aggregated, typically through an averaging operation. This aggregation process creates a single vector representation that captures the context of the target word.\n",
    "\n",
    "**Prediction:** Using this aggregated context vector, CBOW aims to predict the target word. It does this by comparing the context vector to the word vectors in the embedding matrix. The output is a probability distribution over all possible words in the vocabulary, with the highest probability assigned to the word that is most likely to occur in the given context.\n",
    "\n",
    "**Training:** CBOW's objective during training is to adjust the word vectors in the embedding matrix so that the predicted word probabilities match the actual target words in the training data. This is typically done using techniques like stochastic gradient descent (SGD).\n",
    "\n",
    "The primary advantage of CBOW is its efficiency, as it often requires less training data\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.\tExplain SkipGram\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "\n",
    "Skip-gram is another type of word embedding model used in natural language processing (NLP) and deep learning. It's designed to learn word embeddings by predicting the context words (words that are likely to appear in the neighborhood) for a given target word. Skip-gram operates in a manner opposite to Continuous Bag of Words (CBOW), which predicts a target word based on its context. Here's how skip-gram works:\n",
    "\n",
    "**Data Preparation:** Like CBOW, skip-gram also uses a large corpus of text as its training data. Each word in the corpus is represented as a unique numerical identifier.\n",
    "\n",
    "**Context Generation:** For each word in the training data, skip-gram defines a context window of a fixed size. This context window includes the words that surround the target word in the sentence.\n",
    "\n",
    "**Word Embedding:** Skip-gram looks up the word vectors for the target word in the embedding matrix. This matrix contains word vectors that are either pre-trained or initialized with random values and learned during training.\n",
    "\n",
    "**Prediction of Context Words:** The objective of skip-gram is to predict the context words within the given context window based on the target word. It does this by comparing the target word's vector to the word vectors in the embedding matrix.\n",
    "\n",
    "**Training:** During training, skip-gram adjusts the word vectors in the embedding matrix so that they become better at predicting the context words. It uses techniques like stochastic gradient descent (SGD) to minimize the prediction error.\n",
    "\n",
    "Skip-gram's main goal is to learn word embeddings that capture the relationships and semantics between words in a way that words with similar meanings or contexts have similar vector representations. These word embeddings can then be used in various NLP tasks, such as word similarity, sentiment analysis, and machine translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.\tExplain Glove Embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer-**\n",
    "\n",
    "GloVe, which stands for Global Vectors for Word Representation, is a word embedding model used in natural language processing (NLP) and deep learning. GloVe is designed to capture word semantics and relationships by considering the global co-occurrence statistics of words in a large corpus of text. Here's an explanation of how GloVe embeddings work:\n",
    "\n",
    "**Word Co-occurrence Matrix:** The first step in training GloVe is to create a word co-occurrence matrix. This matrix represents how often words co-occur with each other in the training corpus. Rows and columns of the matrix correspond to words in the vocabulary, and the values in the matrix indicate the number of times words appear together in a context window.\n",
    "\n",
    "**Initialization:** GloVe initializes word vectors for each word in the vocabulary. These vectors can be initialized randomly or with pre-trained embeddings.\n",
    "\n",
    "**Objective Function:** GloVe defines an objective function that aims to learn word vectors such that their dot products match the logarithms of the word co-occurrence probabilities. In other words, it tries to ensure that the inner product of word vectors resembles the probability of those words occurring together.\n",
    "\n",
    "**Training:** During training, GloVe optimizes the word vectors to minimize the difference between the dot products of word vectors and the logarithms of co-occurrence probabilities. This is typically done using techniques like gradient descent.\n",
    "\n",
    "**Word Embeddings:** After training, the word vectors obtained from GloVe represent words in a high-dimensional vector space. These vectors capture semantic relationships between words based on their co-occurrence patterns. Words with similar meanings or contexts have similar vector representations.\n",
    "\n",
    "**Vector Arithmetic:** One of the advantages of GloVe embeddings is that you can perform vector arithmetic operations on the word vectors. For example, you can subtract the vector for \"king\" from \"queen\" and add the result to \"man\" to obtain a vector close to \"woman.\" This ability to capture analogies and relationships between words is a valuable feature of GloVe embeddings.\n",
    "\n",
    "GloVe embeddings are known for their ability to capture global word semantics and relationships, making them useful for various NLP tasks, including text classification, sentiment analysis, machine translation, and word similarity calculations. They are especially popular when working with large corpora of text due to their effectiveness in handling word co-occurrence information. GloVe embeddings are available in different dimensions, and you can choose the appropriate dimensionality based on your specific NLP task and computational resources.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
